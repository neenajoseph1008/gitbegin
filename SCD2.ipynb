{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMfxOUXzJDTwEuyAOabe7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neenajoseph1008/gitbegin/blob/main/SCD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiiuunxO-cVC",
        "outputId": "87165ad5-0a78-495d-aefd-4cc4ded45a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SCD2\").getOrCreate()\n",
        "\n",
        "emp_add = spark.read.csv(\"/content/Emp.csv\",inferSchema=True,header=True)\n",
        "emp_add.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHhQs5s-joU",
        "outputId": "96fcd614-f1c5-4e7d-f745-6e74d96242c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+------------+--------+\n",
            "|emp_id| name|address|active|      eff_dt|end_date|\n",
            "+------+-----+-------+------+------------+--------+\n",
            "|     1|Neena|   Pune|     1|'2023-04-01'|  'null'|\n",
            "|     2|Naina|Germany|     1|'2023-04-01'|  'null'|\n",
            "+------+-----+-------+------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "incoming_emp_add = spark.read.csv(\"/content/Emp_12.txt\",inferSchema=True,header=True)\n",
        "incoming_emp_add.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ql1y5U1-_3y",
        "outputId": "573a4679-6b97-4b50-cc34-578ad905aea3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+\n",
            "|emp_id|  name|address|\n",
            "+------+------+-------+\n",
            "|     1| Neena|   Pune|\n",
            "|     2| Naina| Africa|\n",
            "|     3|Akshay|Denmark|\n",
            "+------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join incoming data with existing dimension data on employee_id\n",
        "merged_data = incoming_emp_add.join(emp_add, \"emp_id\", \"left_outer\")\n",
        "\n",
        "merged_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP0UWGUmABqr",
        "outputId": "03e7b61c-6a77-45b0-e6c1-c3895974860e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+-----+-------+------+------------+--------+\n",
            "|emp_id|  name|address| name|address|active|      eff_dt|end_date|\n",
            "+------+------+-------+-----+-------+------+------------+--------+\n",
            "|     1| Neena|   Pune|Neena|   Pune|     1|'2023-04-01'|  'null'|\n",
            "|     2| Naina| Africa|Naina|Germany|     1|'2023-04-01'|  'null'|\n",
            "|     3|Akshay|Denmark| null|   null|  null|        null|    null|\n",
            "+------+------+-------+-----+-------+------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWSeo16sBc1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date, lit, col, when\n",
        "# Define SCD2 logic\n",
        "scd2_data = merged_data.select(\n",
        "    col(\"employee_id\"),\n",
        "    when(col(\"existing_column\") != col(\"incoming_column\"), current_date()).otherwise(col(\"start_date\")).alias(\"start_date\"),\n",
        "    lit(None).cast(\"date\").alias(\"end_date\"),\n",
        "    col(\"incoming_column\").alias(\"new_column\")\n",
        ")"
      ],
      "metadata": {
        "id": "DVdqtyoUAWD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}